@ARTICLE{9380516,
author={Yao, Junmei and Zheng, Xiaolong and Xie, Ruitao and Wu, Kaishun},
journal={IEEE Transactions on Mobile Computing},
title={Cross-Technology Communication for Heterogeneous Wireless Devices through Symbol-Level Energy Modulation},
year={2021},
volume={},
number={},
pages={1-1},
abstract={The coexistence of heterogeneous devices in wireless networks brings a new topic on cross-technology communication (CTC) to improve the coexistence efficiency and boost collaboration among these devices. Current advances on CTC mainly fall into two categories, $physical-layer CTC$ and $packet-level energy modulation$ (PLEM). The $physical-layer CTC$ achieves a high CTC data rate, but with channel incompatible to commercial devices, making it hard to be deployed in current wireless networks. PLEM is channel and physical layer compatible, but with two main drawbacks of the low CTC data rate and MAC incompatibility, which will induce severe interference to the other devices' normal data transmissions. In this paper, we propose symbol-level energy modulation (SLEM), the first CTC method that is fully compatible with current devices in both channel and the physical/MAC layer processes, having the ability to be deployed in commercial wireless networks smoothly. SLEM inserts extra bits to WiFi data bits to generate the transmitting bits, so as to adjust the energy levels of WiFi symbols to deliver CTC information. We make theoretical analysis to figure out performance of both CTC and WiFi transmissions. We also conduct experiments to demonstrate the feasibility of SLEM and its performance under different network situations.},
keywords={Wireless fidelity;Zigbee;Modulation;Receivers;Standards;Wireless networks;Quadrature amplitude modulation;Wireless Networks;Cross-Technology Communication;WiFi;ZigBee},
doi={10.1109/TMC.2021.3065998},
ISSN={1558-0660},
month={},}
@ARTICLE{9325014,
author={Shu, Jiangang and Zou, Xing and Jia, Xiaohua and Zhang, Weizhe and Xie, Ruitao},
journal={IEEE Transactions on Cloud Computing},
title={Blockchain-Based Decentralized Public Auditing for Cloud Storage},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Public auditing schemes for cloud storage systems have been extensively explored with the increasing importance of data integrity. A third-party auditor (TPA) is introduced in public auditing schemes to verify the integrity of outsourced data on behalf of users. To resist malicious TPAs, many blockchain-based public verification schemes have been proposed. However, existing auditing schemes rely on a centralized TPA, and they are vulnerable to tempting auditors who may collude with malicious blockchain miners to produce biased auditing results. In this paper, we propose a blockchain-based decentralized public auditing (BDPA) scheme by utilizing a decentralized blockchain network to undertake the responsibility of a centralized TPA, and also mitigate the influence of tempting auditors and malicious blockchain miners by taking the concept of decentralized autonomous organization (DAO). A detailed security analysis shows that BDPA can preserve data integrity against tempting auditors and malicious blockchain miners. A comprehensive performance evaluation demonstrates that BDPA is feasible and scalable.},
keywords={Cloud computing;Blockchain;Servers;Security;Electronic mail;Computer science;Software;Cloud storage;public integrity auditing;identity-based cryptography;blockchain},
doi={10.1109/TCC.2021.3051622},
ISSN={2168-7161},
month={},}
@ARTICLE{9612025,
author={Xie, Ruitao and Fang, Junhong and Yao, Junmei and Jia, Xiaohua and Wu, Kaishun},
journal={IEEE Transactions on Cloud Computing},
title={Sharing-Aware Task Offloading of Remote Rendering for Interactive Applications in Mobile Edge Computing},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Leveraging emerging mobile edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive applications (e.g. virtual reality and cloud gaming onto GPU-based edge servers to reduce the user experienced latency. A task offloading problem arises, that is where to offload rendering tasks such that each user will experience tolerable delay and meanwhile the cost of used servers is minimized. The multi-dimensional resource sharing feature of rendering tasks makes the problem challenging. We formulate the task offloading problem into a boolean linear programming. We propose a sharing-aware offloading algorithm which decomposes the problem into two subproblems (user assignment and server packing and solves them alternately and iteratively. We compare our algorithm with the one without resource sharing in consideration, and the simulations demonstrate that our method can effectively reduce cost as well as satisfy delay requirement.},
keywords={Rendering (computer graphics);Task analysis;Servers;Delays;Costs;Resource management;Cloud computing;mobile edge computing;task offloading;delay sensitive;multi-dimensional resource sharing;remote rendering;interactive applications},
doi={10.1109/TCC.2021.3127345},
ISSN={2168-7161},
month={},}
@ARTICLE{6975186,
author={Xie, Ruitao and Wen, Yonggang and Jia, Xiaohua and Xie, Haiyong},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Supporting Seamless Virtual Machine Migration via Named Data Networking in Cloud Data Center},
year={2015},
volume={26},
number={12},
pages={3485-3497},
abstract={Virtual machine migration has been touted as one of the crucial technologies in improving data center efficiency, such as reducing energy cost and maintaining load balance. However, traditional approaches could not avoid the service interruption completely. Moreover, they often result in longer delay and are prone to failures. In this paper, we leverage the emerging named data networking (NDN) to design an efficient and robust protocol to support seamless virtual machine migration in cloud data center. Specifically, virtual machines (VMs) are named with the services they provide. Request routing is based on service names instead of IP addresses that are normally bounded with physical machines. As such, services would not be interrupted when migrating supported VMs to different physical machines. We further analyze the performance of our proposed NDN-based VM migration protocol, and optimize its performance via a load balancing algorithm. Our extensive evaluations verify the effectiveness and the efficiency of our approach and demonstrate that it is interruption-free.},
keywords={Cloud computing;Virtual machining;Routing protocols;IP networks;Load management;Named-Data Networking;Cloud Data Center;virtual machine migration;Named-data networking;cloud data center;virtual machine migration},
doi={10.1109/TPDS.2014.2377119},
ISSN={1558-2183},
month={Dec},}
@ARTICLE{6585778,
author={Yang, Kan and Jia, Xiaohua and Ren, Kui and Zhang, Bo and Xie, Ruitao},
journal={IEEE Transactions on Information Forensics and Security},
title={DAC-MACS: Effective Data Access Control for Multiauthority Cloud Storage Systems},
year={2013},
volume={8},
number={11},
pages={1790-1801},
abstract={Data access control is an effective way to ensure data security in the cloud. However, due to data outsourcing and untrusted cloud servers, the data access control becomes a challenging issue in cloud storage systems. Existing access control schemes are no longer applicable to cloud storage systems, because they either produce multiple encrypted copies of the same data or require a fully trusted cloud server. Ciphertext-policy attribute-based encryption (CP-ABE) is a promising technique for access control of encrypted data. However, due to the inefficiency of decryption and revocation, existing CP-ABE schemes cannot be directly applied to construct a data access control scheme for multiauthority cloud storage systems, where users may hold attributes from multiple authorities. In this paper, we propose data access control for multiauthority cloud storage (DAC-MACS), an effective and secure data access control scheme with efficient decryption and revocation. Specifically, we construct a new multiauthority CP-ABE scheme with efficient decryption, and also design an efficient attribute revocation method that can achieve both forward security and backward security. We further propose an extensive data access control scheme (EDAC-MACS), which is secure under weaker security assumptions.},
keywords={Cloud computing;Access control;Servers;Public key;Encryption;Access control;attribute revocation;CP-ABE;decryption outsourcing;multiauthority cloud},
doi={10.1109/TIFS.2013.2279531},
ISSN={1556-6021},
month={Nov},}
@ARTICLE{6487499,
author={Xie, Ruitao and Jia, Xiaohua},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Transmission-Efficient Clustering Method for Wireless Sensor Networks Using Compressive Sensing},
year={2014},
volume={25},
number={3},
pages={806-815},
abstract={Compressive sensing (CS) can reduce the number of data transmissions and balance the traffic load throughout networks. However, the total number of transmissions for data collection by using pure CS is still large. The hybrid method of using CS was proposed to reduce the number of transmissions in sensor networks. However, the previous works use the CS method on routing trees. In this paper, we propose a clustering method that uses hybrid CS for sensor networks. The sensor nodes are organized into clusters. Within a cluster, nodes transmit data to cluster head (CH) without using CS. CHs use CS to transmit data to sink. We first propose an analytical model that studies the relationship between the size of clusters and number of transmissions in the hybrid CS method, aiming at finding the optimal size of clusters that can lead to minimum number of transmissions. Then, we propose a centralized clustering algorithm based on the results obtained from the analytical model. Finally, we present a distributed implementation of the clustering method. Extensive simulations confirm that our method can reduce the number of transmissions significantly.},
keywords={Clustering algorithms;Clustering methods;Data collection;Data communication;Routing;Vectors;Compressed sensing;Wireless sensor networks;compressive sensing;data collection;clustering},
doi={10.1109/TPDS.2013.90},
ISSN={1558-2183},
month={March},}
@INPROCEEDINGS{6848142,
author={Yang, Kan and Jia, Xiaohua and Ren, Kui and Xie, Ruitao and Huang, Liusheng},
booktitle={IEEE INFOCOM 2014 - IEEE Conference on Computer Communications},
title={Enabling efficient access control with dynamic policy updating for big data in the cloud},
year={2014},
volume={},
number={},
pages={2013-2021},
abstract={Due to the high volume and velocity of big data, it is an effective option to store big data in the cloud, because the cloud has capabilities of storing big data and processing high volume of user access requests. Attribute-Based Encryption (ABE) is a promising technique to ensure the end-to-end security of big data in the cloud. However, the policy updating has always been a challenging issue when ABE is used to construct access control schemes. A trivial implementation is to let data owners retrieve the data and re-encrypt it under the new access policy, and then send it back to the cloud. This method incurs a high communication overhead and heavy computation burden on data owners. In this paper, we propose a novel scheme that enabling efficient access control with dynamic policy updating for big data in the cloud. We focus on developing an outsourced policy updating method for ABE systems. Our method can avoid the transmission of encrypted data and minimize the computation work of data owners, by making use of the previously encrypted data with old access policies. Moreover, we also design policy updating algorithms for different types of access policies. The analysis show that our scheme is correct, complete, secure and efficient.},
keywords={Servers;Encryption;Big data;Public key;Access control;Access Control;Policy Updating;ABE;Big Data;Cloud},
doi={10.1109/INFOCOM.2014.6848142},
ISSN={0743-166X},
month={April},}
@ARTICLE{8944075,
author={Xie, Ruitao and Jia, Xiaohua and Wu, Kaishun},
journal={IEEE Journal on Selected Areas in Communications},
title={Adaptive Online Decision Method for Initial Congestion Window in 5G Mobile Edge Computing Using Deep Reinforcement Learning},
year={2020},
volume={38},
number={2},
pages={389-403},
abstract={Mobile edge computing provides users with low response time and avoids unnecessary data transmission. Due to the deployment of 5G, the emerging edge systems can provide gigabit bandwidth. However, network protocols have not evolved together. In TCP, the initial congestion window (IW) is such a low value that most short flows still stay in slow start phase when finishing, and do not fully utilize available bandwidth. Naively increasing IW may result in congestion, which causes long latency. Moreover, since the network environment is dynamic, we have a challenging problem-how to adaptively adjust IW such that flow completion time is optimized, while congestion is minimized. In this paper, we propose an adaptive online decision method to solve the problem, which learns the best policy using deep reinforcement learning stably and fast. In addition, we propose an approach to further improve the performance by supervised learning, using data collected during online learning. We also propose to adopt SDN to address the challenges in implementing our method in MEC systems. To evaluate our method, we build an MEC simulator based on ns3. Our simulations demonstrate that our method performs better than existing methods. It can effectively reduce FCT with little congestion caused.},
keywords={Microsoft Windows;Adaptation models;Servers;Edge computing;5G mobile communication;Reinforcement learning;Bandwidth;Congestion control;initial congestion window;deep reinforcement learning;mobile edge computing;software-defined networking},
doi={10.1109/JSAC.2019.2959187},
ISSN={1558-0008},
month={Feb},}
@INPROCEEDINGS{6679876,
author={Xie, Ruitao and Jia, Xiaohua and Yang, Kan and Zhang, Bo},
booktitle={2013 IEEE 33rd International Conference on Distributed Computing Systems Workshops},
title={Energy Saving Virtual Machine Allocation in Cloud Computing},
year={2013},
volume={},
number={},
pages={132-137},
abstract={In the data center, a server can work in either active state or power-saving state. The power consumption in the power-saving state is almost 0, thus it is always desirable to allocate as many VMs as possible to some active servers and leave the rest to power-saving state in order to reduce the energy consumption of the data center. In this paper, we study such a VM allocation problem. Given a set VMs and a set of servers in a data center, each VM has a resource demand (CPU, memory, storage) and a starting time and a finishing time, and each server has resource capacity. There is an additional energy cost for a server to switch from power-saving state to active state. The servers are non-homogeneous. The problem of our concern is to allocate the VMs onto servers, such that the VMs resource demands can be met and the total energy consumption of servers is minimized. The problem is formulated as a boolean integer linear programming problem. A heuristic algorithm is proposed to solve the problem. Extensive simulations have been conducted to demonstrate our proposed method can significantly save the energy consumption in data centers.},
keywords={Servers;Resource management;Power demand;Energy consumption;Switches;Standards;Memory management;Virtual Machine Allocation;Energy Saving;Cloud Computing;Data Center},
doi={10.1109/ICDCSW.2013.37},
ISSN={2332-5666},
month={July},}
@ARTICLE{9099808,
author={Liu, Chunhui and Liu, Kai and Guo, Songtao and Xie, Ruitao and Lee, Victor C. S. and Son, Sang H.},
journal={IEEE Internet of Things Journal},
title={Adaptive Offloading for Time-Critical Tasks in Heterogeneous Internet of Vehicles},
year={2020},
volume={7},
number={9},
pages={7999-8011},
abstract={With the recent development of wireless communication, sensing, and computing technologies, Internet of Vehicles (IoV) has attracted great attention in both academia and industry. Nevertheless, it is challenging to process time-critical tasks due to unique characteristics of IoV, including heterogeneous computation and communication capacities of network nodes, intermittent wireless connections, unevenly distributed workload, massive data transmission, intensive computation demands, and high mobility of vehicles. In this article, we propose a two-layer vehicular fog computing (VFC) architecture to explore the synergistic effect of the cloud, the static fog, and the mobile fog on processing time-critical tasks in IoV. Then, we give a motivational case study by implementing a prototype of a traffic abnormity detection and warning system, which demonstrates the necessity and urgency of developing adaptive task offloading mechanisms in such a scenario and gives insight into the problem formulation. Furthermore, we formulate the offloading model, aiming at maximizing the completion ratio of time-critical tasks. On this basis, we propose an adaptive task offloading algorithm (ATOA). Specifically, it adaptively categorizes all tasks into four types of pending lists by considering the dynamic requirements and resource constraints, and then tasks in each list will be cooperatively offloaded to different nodes based on their features. Finally, we build the simulation model and give a comprehensive performance evaluation. The results demonstrate the superiority of ATOA.},
keywords={Task analysis;Time factors;Edge computing;Computer architecture;Delays;Wireless communication;Adaptation models;Adaptive offloading;fog computing;Internet of Vehicles (IoV);time-critical task},
doi={10.1109/JIOT.2020.2997720},
ISSN={2327-4662},
month={Sep.},}
@ARTICLE{9204665,
author={Feng, Ruitao and Chen, Sen and Xie, Xiaofei and Meng, Guozhu and Lin, Shang-Wei and Liu, Yang},
journal={IEEE Transactions on Information Forensics and Security},
title={A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices},
year={2021},
volume={16},
number={},
pages={1563-1578},
abstract={Currently, Android malware detection is mostly performed on server side against the increasing number of malware. Powerful computing resource provides more exhaustive protection for app markets than maintaining detection by a single user. However, apart from the applications (apps) provided by the official market (i.e., Google Play Store), apps from unofficial markets and third-party resources are always causing serious security threats to end-users. Meanwhile, it is a time-consuming task if the app is downloaded first and then uploaded to the server side for detection, because the network transmission has a lot of overhead. In addition, the uploading process also suffers from the security threats of attackers. Consequently, a last line of defense on mobile devices is necessary and much-needed. In this paper, we propose an effective Android malware detection system, MobiTive, leveraging customized deep neural networks to provide a real-time and responsive detection environment on mobile devices. MobiTive is a pre-installed solution rather than an app scanning and monitoring engine using after installation, which is more practical and secure. Although a deep learning-based approach can be maintained on server side efficiently for malware detection, original deep learning models cannot be directly deployed and executed on mobile devices due to various performance limitations, such as computation power, memory size, and energy. Therefore, we evaluate and investigate the following key points: (1) the performance of different feature extraction methods based on source code or binary code; (2) the performance of different feature type selections for deep learning on mobile devices; (3) the detection accuracy of different deep neural networks on mobile devices; (4) the real-time detection performance and accuracy on different mobile devices; (5) the potential based on the evolution trend of mobile devices' specifications; and finally we further propose a practical solution (MobiTive) to detect Android malware on mobile devices.},
keywords={Malware;Androids;Humanoid robots;Feature extraction;Mobile handsets;Performance evaluation;Security;Android malware;malware detection;deep neural network;mobile platform;performance},
doi={10.1109/TIFS.2020.3025436},
ISSN={1556-6021},
month={},}
@INPROCEEDINGS{8882775,
author={Feng, Ruitao and Chen, Sen and Xie, Xiaofei and Ma, Lei and Meng, Guozhu and Liu, Yang and Lin, Shang-Wei},
booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)},
title={MobiDroid: A Performance-Sensitive Malware Detection System on Mobile Platform},
year={2019},
volume={},
number={},
pages={61-70},
abstract={Currently, Android malware detection is mostly performed on the server side against the increasing number of Android malware. Powerful computing resource gives more exhaustive protection for Android markets than maintaining detection by a single user in many cases. However, apart from the Android apps provided by the official market (i.e., Google Play Store), apps from unofficial markets and third-party resources are always causing a serious security threat to end-users. Meanwhile, it is a time-consuming task if the app is downloaded first and then uploaded to the server side for detection because the network transmission has a lot of overhead. In addition, the uploading process also suffers from the threat of attackers. Consequently, a last line of defense on Android devices is necessary and much-needed. To address these problems, in this paper, we propose an effective Android malware detection system, MobiDroid, leveraging deep learning to provide a real-time secure and fast response environment on Android devices. Although a deep learning-based approach can be maintained on server side efficiently for detecting Android malware, deep learning models cannot be directly deployed and executed on Android devices due to various performance limitations such as computation power, memory size, and energy. Therefore, we evaluate and investigate the different performances with various feature categories, and further provide an effective solution to detect malware on Android devices. The proposed detection system on Android devices in this paper can serve as a starting point for further study of this important area.},
keywords={Malware;Feature extraction;Computational modeling;Security;Deep learning;Performance evaluation;Servers;Android malware;Malware detection;Deep neural network;Mobile platform},
doi={10.1109/ICECCS.2019.00014},
ISSN={},
month={Nov},}
@ARTICLE{7299311,
author={Zhang, Bo and Jia, Xiaohua and Yang, Kan and Xie, Ruitao},
journal={IEEE Transactions on Vehicular Technology},
title={Design of Analytical Model and Algorithm for Optimal Roadside AP Placement in VANETs},
year={2016},
volume={65},
number={9},
pages={7708-7718},
abstract={The emerging vehicular ad hoc network (VANET) enables vehicles to access the Internet through roadside access points (APs). An important issue in system deployment is determining how many roadside APs shall be installed on a road. However, the existing works fail to provide rigorous and accurate analysis for VANETs. In this paper, we propose a general structure for Internet access in VANETs. It allows both real-time traffic and delay-tolerant traffic to be delivered to users in the most efficient ways. An analytical model is also proposed to analyze the system performance with random arrival of the vehicles. We finally develop an AP placement algorithm based on theoretical results derived from the model to deploy the minimal number of roadside APs with quality-of-service (QoS) guarantees. The simulation results have demonstrated the accuracy of the proposed analytical model and the efficiency of the proposed algorithm.},
keywords={Vehicles;Relays;Real-time systems;Roads;Loading;Throughput;Analytical models;Analytical model;delay-tolerant traffic;real-time traffic;roadside access point (AP) placement;vehicular ad hoc network (VANET)},
doi={10.1109/TVT.2015.2491358},
ISSN={1939-9359},
month={Sep.},}
@INPROCEEDINGS{6134304,
author={Xie, Ruitao and Jia, Xiaohua},
booktitle={2011 IEEE Global Telecommunications Conference - GLOBECOM 2011},
title={Minimum Transmission Data Gathering Trees for Compressive Sensing in Wireless Sensor Networks},
year={2011},
volume={},
number={},
pages={1-5},
abstract={Compressive sensing (CS) can reduce the number of data transmissions and balance the traffic load throughout networks. However, the total number of data transmissions required in CS method is still large. It is observed that there are many zero elements in the measurement matrix. In each round of data transmission in CS method, the sensor nodes corresponding to the zero elements in the measurement matrix do not have their own data to transmit. To further reduce the number of data transmissions in the network, we aim to compute a data gathering tree by taking advantages of these zero elements in the measurement matrix, such that the total number of data transmissions is minimized. We formulate the problem as linear programming with boolean variables. The problem is NP-hard. We propose heuristic algorithm to compute the Minimum Transmission Tree (MTT) for data gathering in CS methods. The MTT algorithm constructs a spanning tree by iteratively including the edge whose average incremental transmission cost is minimum. The simulation results demonstrate that our algorithm can reduce the number of transmissions significantly, compared with the methods using minimum spanning tree (MST), shortest path tree and the CS method with nonzero measurement coefficient using MST.},
keywords={Peer to peer computing;Topology;Data communication;Reliability;Network topology;Lattices;Compressed sensing},
doi={10.1109/GLOCOM.2011.6134304},
ISSN={1930-529X},
month={Dec},}
@ARTICLE{8752530,
author={Xie, Ruitao and Jia, Xiaohua and Wang, Lu and Wu, Kaishun},
journal={IEEE Wireless Communications},
title={Energy Efficiency Enhancement for CNN-based Deep Mobile Sensing},
year={2019},
volume={26},
number={3},
pages={161-167},
abstract={Recently, deep learning has been used to tackle mobile sensing problems, and the inference phase of deep learning is preferred to be run on mobile devices for speedy responses. However, mobile devices are resource-constrained platforms for both computation and power. Moreover, an inference task with deep learning involves tens of billions of mathematical operations and tens of millions of parameter reads. Thus, it is a critical issue to reduce the energy consumption of deep learning inference algorithms. In this article, we survey various energy reduction approaches, and classify them into three categories: the compressing neural network model, minimizing the data transfer required in computation, and offloading workloads. Moreover, we simulate and compare three techniques of model compression, by applying them to an object recognition problem.},
keywords={Sensors;Neural networks;Computational modeling;Data models;Mobile handsets;Neurons;Deep learning;Energy efficiency},
doi={10.1109/MWC.2019.1800321},
ISSN={1558-0687},
month={June},}
@INPROCEEDINGS{7841558,
author={Xie, Ruitao and Umair, Zuneera and Jia, Xiaohua},
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)},
title={A Wireless Solution for SDN (Software Defined Networking) in Data Center Networks},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Software Defined Networking has been adopted to improve data center network efficiency. In SDN, the controllers are responsible for exchanging information with the switches to perform specific operations such as data forwarding. The transmission of control traffic usually uses networks different from data networks. However, building an additional wired network for the control traffic leads to high cabling complexity. Since a wireless network involves almost no cabling and is easy to install, we propose a wireless solution where switches make a wireless connection with controllers via wireless access points (AP). In this design, switches are divided into clusters, and an AP is placed at the center of each cluster. An important issue is to determine the minimum number of APs such that a given control traffic demand can be met. We propose an analytical model to evaluate the system throughput for possible clusterings, and an efficient algorithm to search for the optimal one. The extensive simulations demonstrate that our method can reduce cabling complexity significantly.},
keywords={Wireless communication;Interference;Control systems;Signal to noise ratio;Throughput;Antennas;Computer architecture},
doi={10.1109/GLOCOM.2016.7841558},
ISSN={},
month={Dec},}
@ARTICLE{9193942,
author={Xie, Ruitao and Liu, Jingxin and Cao, Rui and Qiu, Connor S. and Duan, Jiang and Garibaldi, Jon and Qiu, Guoping},
journal={IEEE Transactions on Medical Imaging},
title={End-to-End Fovea Localisation in Colour Fundus Images With a Hierarchical Deep Regression Network},
year={2021},
volume={40},
number={1},
pages={116-128},
abstract={Accurately locating the fovea is a prerequisite for developing computer aided diagnosis (CAD) of retinal diseases. In colour fundus images of the retina, the fovea is a fuzzy region lacking prominent visual features and this makes it difficult to directly locate the fovea. While traditional methods rely on explicitly extracting image features from the surrounding structures such as the optic disc and various vessels to infer the position of the fovea, deep learning based regression technique can implicitly model the relation between the fovea and other nearby anatomical structures to determine the location of the fovea in an end-to-end fashion. Although promising, using deep learning for fovea localisation also has many unsolved challenges. In this paper, we present a new end-to-end fovea localisation method based on a hierarchical coarse-to-fine deep regression neural network. The innovative features of the new method include a multi-scale feature fusion technique and a self-attention technique to exploit location, semantic, and contextual information in an integrated framework, a multi-field-of-view (multi-FOV) feature fusion technique for context-aware feature learning and a Gaussian-shift-cropping method for augmenting effective training data. We present extensive experimental results on two public databases and show that our new method achieved state-of-the-art performances. We also present a comprehensive ablation study and analysis to demonstrate the technical soundness and effectiveness of the overall framework and its various constituent components.},
keywords={Biomedical imaging;Blood vessels;Feature extraction;Machine learning;Visualization;Retina;Image color analysis;Fovea localisation;coarse-to-fine framework;three-stage network;deep learning;data fusion;data augmentation},
doi={10.1109/TMI.2020.3023254},
ISSN={1558-254X},
month={Jan},}
@ARTICLE{7180342,
author={Xie, Ruitao and Jia, Xiaohua},
journal={IEEE Transactions on Cloud Computing},
title={Data Transfer Scheduling for Maximizing Throughput of Big-Data Computing in Cloud Systems},
year={2018},
volume={6},
number={1},
pages={87-98},
abstract={Many big-data computing applications have been deployed in cloud platforms. These applications normally demand concurrent data transfers among computing nodes for parallel processing. It is important to find the best transfer scheduling leading to the least data retrieval time-the maximum throughput in other words. However, the existing methods cannot achieve this, because they ignore link bandwidths and the diversity of data replicas and paths. In this paper, we aim to develop a max-throughput data transfer scheduling to minimize the data retrieval time of applications. Specifically, the problem is formulated into mixed integer programming, and an approximation algorithm is proposed, with its approximation ratio analyzed. The extensive simulations demonstrate that our algorithm can obtain near optimal solutions.},
keywords={Data transfer;Approximation methods;Bandwidth;Approximation algorithms;Radio frequency;Cloud computing;Processor scheduling;Data transfer scheduling;big-data computing;throughput maximization;data center},
doi={10.1109/TCC.2015.2464808},
ISSN={2168-7161},
month={Jan},}
@INPROCEEDINGS{8728907,
author={Feng, Ruitao and Meng, Guozhu and Xie, Xiaofei and Su, Ting and Liu, Yang and Lin, Shang-Wei},
booktitle={2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
title={Learning Performance Optimization from Code Changes for Android Apps},
year={2019},
volume={},
number={},
pages={285-290},
abstract={Performance issues of Android apps can tangibly degrade user experience. However, it is challenging for Android developers, especially a novice to develop high-performance apps. It is primarily attributed to the lack of consolidated and abundant programmatic guides for performance optimization. To address this challenge, we propose a data-based approach to obtain performance optimization practices from historical code changes. We first elicit performance-aware Android APIs of which invocations could affect app performance to a large extent, identify historical code changes that produce impact on app performance, and further determine whether they are optimization practices. We have implemented this approach with a tool \tool and evaluated its effectiveness in 2 open source well-maintained projects. The experimental results found 83 changes relevant to performance optimization. Last, we summarize and explain 5 optimization rules to facilitate the development of high-performance apps.},
keywords={Optimization;Lead;Performance evaluation;Java;Tools;Graphical user interfaces;Programming;Android App;Change Abstraction;Performance Optimization},
doi={10.1109/ICSTW.2019.00067},
ISSN={},
month={April},}
@INPROCEEDINGS{5348543,
author={Yubin, Zhao and Li, Hui and Ruitao, Xie and Yaojun, Qiao and Yuefeng, Ji},
booktitle={2009 2nd IEEE International Conference on Broadband Network & Multimedia Technology},
title={Wireless protection switching for vedio service in wireless-optical broadband access network},
year={2009},
volume={},
number={},
pages={760-764},
abstract={This paper tackles a survivability problem of the Optical-Wireless Broadband Access Network (WOBAN). The proposed scheme introduces the ONU with wireless function (called WONU) for multiple service applications, and uses the wireless resource to protect the network from fiber failure. It establishes the wireless protect link between two WONU for the inter-ONU communication to restore the service when the last-drop fiber is cut. Moreover, to reduce the bandwidth confliction and achieve high effective bandwidth utilization, strategy by using the time-domain adaptive linear prediction and Media Delivery Index (MDI) as the reference to guarantee the QoS is embedded in the scheme. To evaluate this scheme, a WOBAN test-bed is constructed. The experimental results indicate that this scheme could effectively protect the video service by using the wireless resource.},
keywords={Protection switching;Optical network units;Optical fiber devices;Bandwidth;Optical fiber communication;Passive optical networks;Optical fiber networks;Optical fiber testing;Wireless networks;EPON;passive optical network;protection switching;QoS;wireless network},
doi={10.1109/ICBNMT.2009.5348543},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6679897,
author={Zhang, Bo and Jia, Xiaohua and Yang, Kan and Xie, Ruitao},
booktitle={2013 IEEE 33rd International Conference on Distributed Computing Systems Workshops},
title={Multi-path Routing and Stream Scheduling with Spatial Multiplexing and Interference Cancellation in MIMO Networks},
year={2013},
volume={},
number={},
pages={256-261},
abstract={In MIMO networks, there are two transmission strategies, SM (spatial multiplexing) and IC (interference cancellation). The use of SM and IC brings additional complexity for routing and transmission scheduling of the links. Almost all existing works on throughput optimization for MIMO systems are based on the assumption that traffic routing is given and they focus only on stream scheduling, where the performance of the system is limited. In this work, we study the joint optimization problem of routing and stream scheduling for MIMO networks, where the traffic is allowed to be routed through multiple paths to take benefit from concurrent transmissions. The problem is first formulated as an Integer Linear Programming (ILP) problem, and solved optimally by using the column generation approach. An efficient online heuristic algorithm for multi-path routing and stream scheduling with dynamic traffic load is then proposed based on the column generation approach. Extensive simulation results have been conducted to demonstrate the efficiency of the proposed algorithms.},
keywords={Routing;MIMO;Time division multiple access;Throughput;Dynamic scheduling;Integrated circuits;Schedules;MIMO;stream scheduling;stream control;multi-path routing},
doi={10.1109/ICDCSW.2013.38},
ISSN={2332-5666},
month={July},}
@INPROCEEDINGS{9238772,
author={Jie, Huilin and Liu, Kai and Zhang, Hao and Xie, Ruitao and Wu, Weiwei and Guo, Songtao},
booktitle={2020 IEEE/CIC International Conference on Communications in China (ICCC)},
title={AODC: Automatic Offline Database Construction for Indoor Localization in a Hybrid UWB/Wi-Fi Environment},
year={2020},
volume={},
number={},
pages={324-329},
abstract={With the rapid development of mobile terminals and the ever-expanding deployment of wireless infrastructures, the requirement of location-based services (LBS) has permeated majority industries. However, fingerprint-based localization method has a bottleneck, namely, the offline database construction is time-consuming and labor-intensive, which hampers its implementation and adaption. Moreover, the mismatch of fingerprint due to various environmental interferences during the online localization phase is another critical issue to be addressed. In view of this, we consider the fingerprint-based indoor localization in a hybrid UWB/WiFi environment, which integrates UWB and Wi-Fi to speed up the construction of offline database, while maintaining a meter-level localization accuracy. Specifically, the system does not require manual labeling efforts for reference points (RP). Instead, we propose a heterogeneous data synchronization scheme (HDSS) to integrate the RSSI data obtained by the Wi-Fi device and the corresponding coordinates obtained by UWB device. Then, an automatic radio map generation scheme (ARMGS) is proposed, which can automatically generate the fingerprints profiles of RPs. Furthermore, for the online localization, unlike traditional approaches, which estimate locations only based on features of signal space, we proposed a dual-domain constraints localization algorithm (DCLA), which takes the physical space into consideration at the same time based on mean shift clustering algorithm. Finally, we implement the prototype of AODC system and carry out extensive real-world experiments. The experimental results validate the effectiveness of the proposed solutions.},
keywords={Wireless communication;Databases;Clustering algorithms;Prototypes;Fingerprint recognition;Synchronization;Wireless fidelity;Wi-Fi Fingerprint;UWB;Dual-domain Constraints;Indoor Localization},
doi={10.1109/ICCC49849.2020.9238772},
ISSN={2377-8644},
month={Aug},}
@ARTICLE{9880929,
author={Zhu, Zhengjie and Yang, Xiaogang and Lu, Ruitao and Shen, Tong and Xie, Xueli and Zhang, Tao},
journal={IEEE Transactions on Instrumentation and Measurement},
title={CLF-Net: Contrastive Learning for Infrared and Visible Image Fusion Network},
year={2022},
volume={},
number={},
pages={1-1},
abstract={In this article, we propose an effective infrared and visible image fusion network based on contrastive learning, which is called CLF-Net. A novel noise contrastive estimation framework is introduced into the image fusion to maximize mutual information between the fused image and source images. First, an unsupervised contrastive learning framework is constructed to promote fused image selectively retaining the most similar features in local areas of different source images. Second, we design a robust contrastive loss based on the deep representations of images, combining with the structural similarity loss to effectively guide the network in extracting and reconstructing features. Specifically, based on the deep representation similarities and structural similarities between the fused image and source images, the loss functions can guide the feature extraction network in adaptively obtaining the salient targets of infrared images and background textures of visible images. Then, the features are reconstructed in the most appropriate manner. In addition, our method is an unsupervised end-to-end model. All of our methods have been tested on public datasets. Based on extensive qualitative and quantitative analysis results, it has been demonstrated that our proposed method performs better than the existing state-of-the-art fusion methods.},
keywords={Feature extraction;Task analysis;Image fusion;Image reconstruction;Computer vision;Estimation;Visualization;Contrastive learning;unsupervised learning;image fusion;infrared image;noise contrastive estimation},
doi={10.1109/TIM.2022.3203000},
ISSN={1557-9662},
month={},}
@ARTICLE{9837949,
author={Wu, Bozhi and Liu, Shangqing and Feng, Ruitao and Xie, Xiaofei and Siow, Jingkai and Lin, Shang-Wei},
journal={IEEE Transactions on Dependable and Secure Computing},
title={Enhancing Security Patch Identification by Capturing Structures in Commits},
year={2022},
volume={},
number={},
pages={1-15},
abstract={With the rapid increasing number of open source software (OSS), the majority of the software vulnerabilities in the open source components are fixed silently, which leads to the deployed software that integrated them being unable to get a timely update. Hence, it is critical to design a security patch identification system to ensure the security of the utilized software. However, most of the existing works for security patch identification just consider the changed code and the commit message of a commit as a flat sequence of tokens with simple neural networks to learn its semantics, while the structure information is ignored. To address these limitations, in this paper, we propose our well-designed approach E-SPI, which extracts the structure information hidden in a commit for effective identification. Specifically, it consists of the code change encoder to extract the syntactic of the changed code with the BiLSTM to learn the code representation and the message encoder to construct the dependency graph for the commit message with the graph neural network (GNN) to learn the message representation. We further enhance the code change encoder by embedding contextual information related to the changed code. To demonstrate the effectiveness of our approach, we conduct the extensive experiments against six state-of-the-art approaches on the existing dataset and from the real deployment environment. The experimental results confirm that our approach can significantly outperform current state-of-the-art baselines.},
keywords={Codes;Security;Syntactics;Semantics;Graph neural networks;Data mining;Task analysis;Security Patch Identification;Graph Neural Networks;Abstract Syntax Tree},
doi={10.1109/TDSC.2022.3192631},
ISSN={1941-0018},
month={},}
@ARTICLE{9864088,
author={Liu, Kai and Jin, Feiyu and Hu, Junbo and Xie, Ruitao and Gu, Fuqiang and Guo, Songtao and Luo, Jiangtao},
journal={IEEE Transactions on Mobile Computing},
title={Towards Robust WiFi Fingerprint-Based Vehicle Tracking in Dynamic Indoor Parking Environments: An Online Learning Framework},
year={2022},
volume={},
number={},
pages={1-15},
abstract={The variation of wireless signal in dynamic indoor parking environments may seriously compromise the performance of fingerprint-based localization methods. In this regard, this paper investigates the problem of robust WiFi fingerprint-based vehicle tracking in dynamic indoor parking environments, aiming at designing an online learning framework to continuously train the localization model and counteract the effect of signal variation. Specifically, a Hidden Markov Model (HMM) based Online Evaluation (HOE) method is firstly proposed to assess the accuracy of localization results by measuring the inconsistency of locations inferred by WiFi fingerprinting and Dead Reckoning (DR). Further, an Online Transfer Learning (OTL) algorithm is designed to improve the robustness of the fingerprinting localization, which consists of a weight allocation scheme to combine two classification models (i.e., the batch model and the online model) and an instance-based transferring scheme to resample the offline fingerprints and retrain the batch model. Finally, we implement the system prototype and give comprehensive performance evaluation, which demonstrates that the proposed solutions can outperform the state-of-the-art localization algorithms around 28% $\sim$ 58% on vehicle tracking accuracy in dynamic indoor parking environments.},
keywords={Hidden Markov models;Location awareness;Wireless fidelity;Vehicle dynamics;Databases;Fingerprint recognition;Training;Hidden markov model;indoor localization;online learning;vehicle tracking;WiFi fingerprint},
doi={10.1109/TMC.2022.3200411},
ISSN={1558-0660},
month={},}
@ARTICLE{9882384,
author={Li, Siqi and Xie, Xiaofei and Lin, Yun and Li, Yuekang and Feng, Ruitao and Li, Xiaohong and Ge, Weimin and Dong, Jin Song},
journal={IEEE Transactions on Dependable and Secure Computing},
title={Deep Learning for Coverage-Guided Fuzzing: How Far are We?},
year={2022},
volume={},
number={},
pages={1-13},
abstract={Fuzzing is a widely-used software vulnerability discovery technology, many of which are optimized using coverage-feedback. Recently, some techniques propose to train deep learning (DL) models to predict the branch coverage of an arbitrary input owing to its always-available gradients etc. as a guide. Those techniques have proved their success in improving coverage and discovering bugs under different experimental settings. However, DL models, usually as a magic black-box, are notoriously lack of explanation. Moreover, their performance can be sensitive to the collected runtime coverage information for training, indicating potentially unstable performance. In this work, we conduct a systematic empirical study on 4 types of DL models across 6 projects to (1) revisit the performance of DL models on predicting branch coverage (2) demystify what specific knowledge do the models exactly learn, (3) study the scenarios where the DL models can outperform and underperform the traditional fuzzers, and (4) gain insight into the challenges of applying DL models on fuzzing. Our empirical results reveal that existing DL-based fuzzers do not perform well as expected, which is largely affected by the dependencies between branches, unbalanced sample distribution, and the limited model expressiveness. In addition, the estimated gradient information tends to be less helpful in our experiments. Finally, we further pinpoint the research directions based on our summarized challenges.},
keywords={Fuzzing;Predictive models;Deep learning;Data models;Computational modeling;Training data;Training;Coverage;deep learning;fuzzing;mutation;testing},
doi={10.1109/TDSC.2022.3200525},
ISSN={1941-0018},
month={},}
@ARTICLE{9767696,
author={Xie, Ruitao and Fang, Junhong and Yao, Junmei and Liu, Kai and Jia, Xiaohua and Wu, Kaishun},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={QoS-Aware Scheduling of Remote Rendering for Interactive Multimedia Applications in Edge Computing},
year={2022},
volume={33},
number={12},
pages={3816-3832},
abstract={Leveraging emerging edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive multimedia applications (e.g., virtual reality and cloud gaming) onto edge servers. For high resource utilization, multiple rendering tasks run in the same GPU server and compete against each other for the computation resource. Each task has its requirement for performance, i.e., QoS target. A significant problem is how to schedule tasks so that each preset QoS is met and the performance of all tasks are maximized. We make the following contributions. First, we formulate the problem into a QoS constrained max-min utility problem. Second, we find that using the common natural logarithm as a utility function overly promotes one performance but demotes another. To avoid this phenomenon, we design a special utility function. Third, we propose an efficient scheduling algorithm, consisting of a resolution adjustment algorithm and a frame rate fair scheduling algorithm, both of which interact with each other. The former selects resolutions for tasks and the latter decides which task to process. We evaluate our method with actual rendering data, and the simulations demonstrate that our method can effectively improve task performance as well as satisfy QoS simultaneously.},
keywords={Rendering (computer graphics);Task analysis;Servers;Quality of service;Streaming media;Graphics processing units;Three-dimensional displays;Edge computing;task scheduling;remote rendering},
doi={10.1109/TPDS.2022.3172121},
ISSN={1558-2183},
month={Dec},}
@ARTICLE{9851558,
author={Sun, Zaixing and Zhang, Boyu and Gu, Chonglin and Xie, Ruitao and Qian, Bin and Huang, Hejiao},
journal={IEEE Transactions on Services Computing},
title={ET2FA: A Hybrid Heuristic Algorithm for Deadline-constrained Workflow Scheduling in Cloud},
year={2022},
volume={},
number={},
pages={1-14},
abstract={Cloud computing is an emerging computational infrastructure for cost-efficient workflow execution that provides flexible and dynamically scalable computing resources at pay-as-you-go pricing. Workflow scheduling, as a typical NP-Complete problem, is one of the major issues in cloud computing. However, in the cloud scenario with unlimited resources, how to generate an efficient and economical workflow scheduling scheme under the deadline constraint is still an extraordinary challenge. In this paper, we propose a hybrid heuristic algorithm called enhanced task type first algorithm (ET2FA) to solve deadline-constrained workflow scheduling in cloud with new features such as hibernation and per-second billing. The objectives to be minimized include the total cost and total idle rate. ET2FA involves three phases: 1) Task type first algorithm, which schedules tasks based on topological level and task types, and utilizes a compact-scheduling-condition based VM selection method to assign each task. 2) Delay operation based on block structure, which further optimizes total cost and total idle rate based on block structure properties. 3) Instance hibernate scheduling heuristic, which sets an instance to hibernate if idle for a duration. Extensive simulation experiments based on seven well-known real-world workflow applications show that ET2FA delivers better performance in comparison to the state-of-the-art algorithms.},
keywords={Task analysis;Scheduling;Heuristic algorithms;Costs;Cloud computing;Job shop scheduling;Computational modeling;Workflow scheduling;cloud computing;deadline constraint;directed acyclic graph;hibernate instance},
doi={10.1109/TSC.2022.3196620},
ISSN={1939-1374},
month={},}
@ARTICLE{9290447,
author={Shu, Jiangang and Wang, Songlei and Jia, Xiaohua and Zhang, Weizhe and Xie, Ruitao and Huang, Hejiao},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={Efficient Lane-Level Map Building via Vehicle-Based Crowdsourcing},
year={2022},
volume={23},
number={5},
pages={4049-4062},
abstract={By providing rich context of lane information on roads, lane-level maps play a vital role in intelligent transportation systems. Since Global Positioning Systems (GPS) have been widely applied to vehicles, vehicle-based crowdsourcing offers an economical way to the lane-level map building by collecting and analyzing the GPS trajectories of vehicles. However, existing works cannot directly extract lane-level road information from raw and interleaved crowdsourcing trajectories, and moreover they are time-consuming and inaccurate. In this article, we propose a lane-level map building scheme, which can directly extract lane-level road information from raw crowdsourcing GPS trajectories with both efficiency and accuracy improvement. Consider the global similarity between trajectories, we design an efficient trajectory segmentation and clustering algorithm based on improved discrete Frchet distance and entropy theory, which can directly and accurately deal with the interleaved and messy trajectories. To improve the efficiency, we employ the Least Square Estimate (LSE) to constrain Gaussian Mixture Model (GMM) and design an efficient and accurate lane-level road information extraction algorithm. Comprehensive comparative experiments and performance evaluation on a real-world trajectory dataset show that the proposed scheme outperforms the state-of-the-art works in terms of both efficiency and accuracy.},
keywords={Trajectory;Global Positioning System;Roads;Crowdsourcing;Task analysis;Data mining;Laser radar;Crowdsourcing;vehicle;lane-level map;trajectory},
doi={10.1109/TITS.2020.3040728},
ISSN={1558-0016},
month={May},}